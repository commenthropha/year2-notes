When the input size doubles, the algorithm should only slow down by some constant factor C (choose C = 2^d)

i.e. There exists constants c > 0 and d > 0 such that on every input of size N, its running time is bounded by cN^d steps.